import tensorflow as tf
import numpy as np
import math
import collections
import pandas as pd
import random


sample_size =32
discount = 0.9

nLink=77
nbatch = 32
nphase = 48
maskQb = tf.Variable(np.zeros([nbatch,1,nphase]), dtype=tf.float32)

def fout(m, x):
    a= tf.einsum("iab,ib->ia", m, x)
    return a

global maskQb    
def Model_set():
    x_ = tf.keras.Input(shape=(77,2))        
    x_ = tf.keras.layers.Flatten()(x_)
       
    x_ = tf.keras.layers.Dense(300)(x_)
    x_ = tf.keras.layers.BatchNormalization()(x_)
    x_ = tf.keras.layers.LeakyReLU(alpha=0.3)(x_) 
    
    x_ = tf.keras.layers.Dense(150)(x_)
    x_ = tf.keras.layers.BatchNormalization()(x_)
    x_ = tf.keras.layers.LeakyReLU(alpha=0.3)(x_) 
    
    x_ = tf.keras.layers.Dense(70)(x_)
    x_ = tf.keras.layers.BatchNormalization()(x_)
    x_ = tf.keras.layers.LeakyReLU(alpha=0.3)(x_)     

    x_ = tf.keras.layers.Dense(48)(x_)
    x_ = tf.keras.activations.tanh(x_)
 
    x_Qb = x_
    x_Qt = x_  
    x_Qt = tf.expand_dims(x_Qt,1) 

    x_Qb = fout(maskQb, x_Qb)
    
    final_model = tf.keras.Model(inputs = x_input, outputs = [x_Qt, x_Qb] )

    return final_model

model = Model_set()
modelT = Model_set()
nbatch = 32
nphase = 48
dqn_variable = model.trainable_variables
Adam = tf.keras.optimizers.Adam() 

def DQN_train(replays):
    sample_size = 32
    replay = replays 
    
    SS = tf.convert_to_tensor(np.asarray(replay[0]))
    action = np.asarray(replay[1])
    rr  = tf.convert_to_tensor(np.asarray(replay[2]), dtype=tf.float32)
    SS_ = tf.convert_to_tensor(np.asarray(replay[3]))
    St = np.asarray(replay[4][3]) #for inference
    
    Qq, _ = modelT(SS_)
    with tf.GradientTape() as tape:
        #tape.watch(dqn_variable)
        QQ = tf.stop_gradient(Qq)
        QQt = [tf.math.reduce_max(QQ[:,:,0:3], axis=2)+ tf.math.reduce_max(QQ[:,:,3:5], axis=2)+ tf.math.reduce_max(QQ[:,:,5:9], axis=2)+ tf.math.reduce_max(QQ[:,:,9:12], axis=2)+ tf.math.reduce_max(QQ[:,:,12:15], axis=2)+\
                 tf.math.reduce_max(QQ[:,:,15:19], axis=2)+ tf.math.reduce_max(QQ[:,:,19:22], axis=2)+ tf.math.reduce_max(QQ[:,:,22:26], axis=2)+ tf.math.reduce_max(QQ[:,:,26:29], axis=2)+ tf.math.reduce_max(QQ[:,:,29:32], axis=2)+\
                 tf.math.reduce_max(QQ[:,:,32:35], axis=2)+ tf.math.reduce_max(QQ[:,:,35:39], axis=2)+ tf.math.reduce_max(QQ[:,:,39:41], axis=2)+ tf.math.reduce_max(QQ[:,:,41:44], axis=2)+ tf.math.reduce_max(QQ[:,:,44:48], axis=2)] # Select a(t)
        QQt= tf.convert_to_tensor(QQt)
        Qt = tf.reshape(rr,[32,1]) + discount * tf.reshape(QQt,[32,1])  #32*1, Qt y값
        
        QQb=[]
        for i in range(32):
            actQ = Mmask(action[i])
            QQb.append(actQ)
        QQb=tf.convert_to_tensor(QQb, dtype=tf.float32)    #a에 해당하는 mask 값
        #print('Qmask: ', QQb[0][0][0:7])
        maskQb.assign(QQb)  # Q값을 위한 mask 변경
        _, Q1 = model(SS)
        
        #loss = tf.reduce_mean((Q1 - Qt)**2))
        loss = tf.keras.losses.MSE(Qt, Q1)
    
    dqn_grads = tape.gradient(loss, dqn_variable)
    Adam.apply_gradients(zip(dqn_grads, dqn_variable))
    
    SSa=np.repeat(np.expand_dims(St,0), 32, axis=0)
    Q1, Q2 = model.predict(SSa, batch_size=32, verbose=0)  #마지막 학습된 모형으로 Q값 뽑아내기.
    Qv= Q1[0:1] 
    return Qv
    

def Mmask(q):
    x=np.zeros([1,48])
    x[0][q[0]]=1
    x[0][q[1]+3]=1
    x[0][q[2]+5]=1
    x[0][q[3]+9]=1
    x[0][q[4]+12]=1
    x[0][q[5]+15]=1
    x[0][q[6]+19]=1
    x[0][q[7]+22]=1
    x[0][q[8]+26]=1
    x[0][q[9]+29]=1
    x[0][q[10]+32]=1
    x[0][q[11]+35]=1
    x[0][q[12]+39]=1
    x[0][q[13]+41]=1
    x[0][q[14]+44]=1
    return x
#%%
nbatch = 32
nphase = 48
